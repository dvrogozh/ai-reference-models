fp16-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-recommendation-pytorch-flex-gpu-dlrm-v1-inference
  cmd: bash run_model.sh
  ipc: host
  device: /dev/dri
  env:
    MULTI_TILE: 'False'
    PLATFORM: 'Flex'
    PRECISION: fp16
    DATASET_DIR: /local_dateset/datasets/Cosim_test/dataset/dlrm_kaggle
    CKPT_DIR: /local_dateset/datasets/Cosim_test/checkpoints/dlrm_kaggle
    OUTPUT_DIR: /tmp
  volumes:
    - src: /local_dateset/datasets/Cosim_test/dataset/dlrm_kaggle
      dst: /local_dateset/datasets/Cosim_test/dataset/dlrm_kaggle
    - src: /local_dateset/datasets/Cosim_test/checkpoints/dlrm_kaggle
      dst: /local_dateset/datasets/Cosim_test/checkpoints/dlrm_kaggle
    - src: /tmp
      dst: /tmp
